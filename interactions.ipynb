{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paths import Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/23 19:13:11 WARN Utils: Your hostname, gr00stl-Legion-Y540-15IRH-PG0 resolves to a loopback address: 127.0.1.1; using 192.168.1.43 instead (on interface wlp0s20f3)\n",
      "23/10/23 19:13:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/23 19:13:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "[Stage 1:=======================================================> (32 + 1) / 33]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- book_id: integer (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "app_name = 'interactions'\n",
    "# spark = SparkSession.builder.appName(app_name).getOrCreate()\n",
    "spark = SparkSession.builder \\\n",
    ".appName(app_name) \\\n",
    ".master(\"local[*]\") \\\n",
    ".config(\"spark.driver.memory\", \"12g\") \\\n",
    ".config(\"spark.sql.debug.maxToStringFields\", 1000) \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Paths.data\n",
    "\n",
    "interactions_df = spark.read.csv(f\"{data_path}goodreads_interactions.csv\", header=True, inferSchema=True).select(\n",
    "    'user_id',\n",
    "    'book_id',\n",
    "    'rating'\n",
    ").limit(10000)\n",
    "\n",
    "interactions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+\n",
      "|user_id|book_id|rating|\n",
      "+-------+-------+------+\n",
      "|      0|    948|     5|\n",
      "|      0|    947|     5|\n",
      "|      0|    946|     5|\n",
      "|      0|    945|     5|\n",
      "|      0|    944|     5|\n",
      "+-------+-------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2805"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter interactions to only include ratings\n",
    "ratings_df = interactions_df.filter(interactions_df.rating != 0)\n",
    "ratings_df.show(5)\n",
    "ratings_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:\n",
      " 2.455986167634981\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing\n",
    "# 0.3/0.7\n",
    "train_data, test_data = ratings_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Alternating Least Squares\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "als = ALS(\n",
    "    userCol='user_id',\n",
    "    itemCol='book_id',\n",
    "    ratingCol='rating',\n",
    "    nonnegative=True,\n",
    "    coldStartStrategy='drop',\n",
    "    maxIter=5,\n",
    "    rank=4,\n",
    "    regParam=0.01,\n",
    ")\n",
    "\n",
    "model = als.fit(train_data)\n",
    "\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName='rmse', labelCol='rating', predictionCol='prediction')\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print('RMSE:\\n', rmse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:\n",
      " 1.4922181968402317\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# parameter grid\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(als.rank, [10, 50, 100, 150]) \\\n",
    "    .addGrid(als.regParam, [.01, .05, .1, .15]) \\\n",
    "    .build()\n",
    "\n",
    "# cross validation\n",
    "cv = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# best model\n",
    "model = cv.fit(train_data)\n",
    "\n",
    "# make predictions on test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# calculate RMSE on test data\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print('RMSE:\\n', rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id:\n",
      " 7\n",
      "+-------+-------+----------+\n",
      "|book_id|user_id|prediction|\n",
      "+-------+-------+----------+\n",
      "|   7247|      7|  4.931852|\n",
      "|   7244|      7|  4.931852|\n",
      "|   7220|      7|  4.931852|\n",
      "|   7240|      7|  4.931852|\n",
      "|   7231|      7|  4.931852|\n",
      "+-------+-------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Recommend book to user by user_id\n",
    "# select random user_id\n",
    "from pyspark.sql.functions import rand\n",
    "\n",
    "user_id = ratings_df.select('user_id').distinct().orderBy(rand()).first().user_id\n",
    "print('user_id:\\n', user_id)\n",
    "\n",
    "# filter out books that user has already rated\n",
    "books_already_rated = ratings_df.filter(ratings_df.user_id == user_id).select('book_id', 'user_id')\n",
    "\n",
    "# recommend books to user\n",
    "recommendations = model.transform(books_already_rated).orderBy('prediction', ascending=False)\n",
    "\n",
    "# get recommended book_ids\n",
    "recommendations.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+\n",
      "|book_id|               title|prediction|\n",
      "+-------+--------------------+----------+\n",
      "|   7240|The Prodigal Son ...|  4.931852|\n",
      "|   7238|Prodigal Son (Dea...|  4.931852|\n",
      "|   7244|The Poisonwood Bible|  4.931852|\n",
      "|   7220|The Secret Life o...|  4.931852|\n",
      "|   7247|Barbara Kingsolve...|  4.931852|\n",
      "+-------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get book titles using book_id in recommendations, search in books_sample.parquet\n",
    "\n",
    "# 1.1 read books_sample.parquet\n",
    "books_df = spark.read.parquet(f\"{data_path}books.parquet\").select('book_id', 'title')\n",
    "\n",
    "# 1.2 join recommendations with books_df on column book_id, select columns book_id, title\n",
    "recommendations_df = recommendations.join(books_df, on='book_id', how='inner').select('book_id', 'title', 'prediction')\n",
    "\n",
    "# 1.3 show top 5 books -> sort by prediction\n",
    "recommendations_df.orderBy('prediction', ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+\n",
      "|user_id|book_id|rating|\n",
      "+-------+-------+------+\n",
      "|      7|    536|     5|\n",
      "|      7|   1387|     5|\n",
      "|      7|   1386|     5|\n",
      "|      7|    417|     5|\n",
      "|      7|   7247|     5|\n",
      "|      7|   7244|     5|\n",
      "|      7|   7243|     5|\n",
      "|      7|   1574|     5|\n",
      "|      7|   7241|     5|\n",
      "|      7|   7240|     5|\n",
      "|      7|   7238|     5|\n",
      "|      7|   5770|     5|\n",
      "|      7|   7231|     5|\n",
      "|      7|   7220|     5|\n",
      "|      7|    197|     5|\n",
      "|      7|   1525|     4|\n",
      "|      7|   7242|     4|\n",
      "|      7|   1605|     4|\n",
      "|      7|   7232|     4|\n",
      "|      7|   7211|     4|\n",
      "|      7|   7250|     3|\n",
      "|      7|    821|     3|\n",
      "|      7|    461|     3|\n",
      "|      7|   7236|     3|\n",
      "|      7|   7219|     3|\n",
      "|      7|   7213|     3|\n",
      "|      7|   1543|     3|\n",
      "|      7|   7181|     3|\n",
      "|      7|   7176|     3|\n",
      "|      7|   7208|     2|\n",
      "|      7|   7233|     1|\n",
      "|      7|   1604|     1|\n",
      "|      7|   7198|     1|\n",
      "+-------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show books read by user (user_id)\n",
    "ratings_df.filter(ratings_df['user_id'] == user_id).orderBy('rating', ascending=False).show(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.7979  0.7704  0.8045  0.8459  0.7986  0.8034  0.0243  \n",
      "MAE (testset)     0.6416  0.6119  0.6304  0.6510  0.6389  0.6348  0.0132  \n",
      "Fit time          0.03    0.02    0.02    0.02    0.02    0.02    0.00    \n",
      "Test time         0.00    0.00    0.00    0.00    0.00    0.00    0.00    \n",
      "book_id:\n",
      " 2980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Prediction(uid=7, iid=2980, r_ui=None, est=3.595612404313358, details={'was_impossible': False})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Surprise reader\n",
    "from surprise import Reader, Dataset, SVD\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "# Load the dataset using the reader\n",
    "data = Dataset.load_from_df(ratings_df.toPandas(), reader)\n",
    "\n",
    "# Use the SVD algorithm.\n",
    "svd = SVD()\n",
    "\n",
    "# Run 5-fold cross-validation and then print results\n",
    "cross_validate(svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
    "\n",
    "# train on whole dataset\n",
    "trainset = data.build_full_trainset()\n",
    "svd.fit(trainset)\n",
    "\n",
    "# predict rating for user_id and book_id\n",
    "random_book_id = ratings_df.select('book_id').distinct().orderBy(rand()).first().book_id\n",
    "print('book_id:\\n', random_book_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
